{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5773fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old old old\n",
    "#print(X.shape)\n",
    "scores_gb = []\n",
    "scores_rf = []\n",
    "\n",
    "counter = 0\n",
    "#print(\"shape\")\n",
    "#print(len(years))\n",
    "#print(X.shape)\n",
    "#17 mow cuz sth isnt working with the 2017 and further, not many vals, the table is complete up to 2016\n",
    "#22 cuz we have not defined year of production for some games\n",
    "for i in range(1980,2017):  \n",
    "    indices = [k for k, x in enumerate(years) if x == i]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[indices[0]:indices[len(indices) - 1]], y[indices[0]:indices[len(indices) - 1]], random_state=0)\n",
    "    reg = GradientBoostingRegressor(loss=\"quantile\", random_state=0)\n",
    "    \n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = reg.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - y_test)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / y_test)\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    scores_gb.append(accuracy)\n",
    "    #center and standardize data, give it a goddamned try\n",
    "    #https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[indices[0]:indices[len(indices) - 1]], y[indices[0]:indices[len(indices) - 1]], random_state=0)\n",
    "    regr = RandomForestRegressor(max_depth=8, random_state=0)\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - y_test)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / y_test)\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')\n",
    "    scores_rf.append(accuracy)\n",
    "        \n",
    "\n",
    "predictions_data_gb = regr.predict(X)\n",
    "yrs  = unique(years)\n",
    "sum_by_years_actual_gb = []\n",
    "sum_by_years_pred_gb = []\n",
    "for i in yrs:\n",
    "    indices = [k for k, x in enumerate(years) if x == i]\n",
    "    sum_by_years_actual_gb.append(np.sum(y[indices[0]:indices[len(indices) - 1]]))\n",
    "    sum_by_years_pred_gb.append(np.sum(predictions_data_gb[indices[0]:indices[len(indices) - 1]]))\n",
    "    \n",
    "    \n",
    "plt.plot(vgsales['Year'], y, 'b-', label = 'actual')\n",
    "plt.plot(vgsales['Year'], predictions_data_gb, 'ro', label = 'prediction')\n",
    "plt.legend()\n",
    "plt.xlabel('Date'); plt.ylabel('Games sales (mln$)'); plt.title('Actual and Predicted Values');\n",
    "\n",
    "mse = metrics.mean_squared_error(y_test, regr.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ff7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# function to get unique values\n",
    "def unique(list1):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "     \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "reg = GradientBoostingRegressor(loss=\"quantile\",random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = reg.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "#center and standardize data, give it a goddamned try\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "regr = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = regr.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        \n",
    "# Get numerical feature importances\n",
    "importances = list(regr.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_list = ['platforms', 'years', 'genres', 'publishers']\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "\n",
    "\n",
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators= 3000, random_state=42)\n",
    "# Extract the two most important features\n",
    "important_indices = [feature_list.index('genres'), feature_list.index('years')]\n",
    "train_important = X_train[:, important_indices]\n",
    "test_important = X_test[:, important_indices]\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, y_train)\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - y_test)\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "\n",
    "predictions_data = rf_most_important.predict(X[:, important_indices])\n",
    "\n",
    "\n",
    "\n",
    "yrs  = unique(years)\n",
    "sum_by_years_actual = []\n",
    "sum_by_years_pred = []\n",
    "for i in yrs:\n",
    "    indices = [k for k, x in enumerate(years) if x == i]\n",
    "    sum_by_years_actual.append(np.sum(y[indices[0]:indices[len(indices) - 1]]))\n",
    "    sum_by_years_pred.append(np.sum(predictions_data[indices[0]:indices[len(indices) - 1]]))\n",
    "    \n",
    "    \n",
    "# Plot the actual values\n",
    "plt.plot(yrs, sum_by_years_actual, 'b-', label = 'actual')\n",
    "# Plot the predicted values\n",
    "plt.plot(yrs, sum_by_years_pred, 'ro', label = 'prediction')\n",
    "#plt.xticks(rotation = '60'); \n",
    "plt.legend()\n",
    "# Graph labels\n",
    "plt.xlabel('Date'); plt.ylabel('Games sales (mln$)'); plt.title('Actual and Predicted Values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "#this piece of code shows how we were calculating the predictions in the first place \n",
    "#and then we switched to training not by parts of unmodified data set, but by years of games production\n",
    "#as we believe that it is more revelant economics-wise. It also makes more sense to do such a thing\n",
    "#from the scientific point of view, since the number of games is growing rapidly from year to year.\n",
    "#The first year in the aforementioned data set is 1980, whilst the last is 2020.\n",
    "\n",
    "scores_gb = []\n",
    "scores_rf = []\n",
    "for i in range(0,17):    \n",
    "    if i != 16:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[1000*i:1000*i+1000], y[1000*i:1000*i+1000], random_state=0)\n",
    "        reg = GradientBoostingRegressor(loss=\"quantile\",random_state=0)\n",
    "        reg.fit(X_train, y_train)\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = regr.predict(X_test)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / y_test)\n",
    "        # Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        scores_gb.append(accuracy)\n",
    "\n",
    "        #center and standardize data, give it a goddamned try\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[1000*i:1000*i+1000], y[1000*i:1000*i+1000], random_state=0)\n",
    "        regr = RandomForestRegressor(max_depth=8, random_state=0)\n",
    "        regr.fit(X_train, y_train)\n",
    "\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = regr.predict(X_test)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / y_test)\n",
    "        # Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        scores_rf.append(accuracy)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[16000:16596], y[16000:16596], random_state=0)\n",
    "        reg = GradientBoostingRegressor(loss=\"quantile\",random_state=0)\n",
    "        reg.fit(X_train, y_train)\n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = reg.predict(X_test)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / y_test)\n",
    "        # Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        scores_gb.append(accuracy)\n",
    "        #center and standardize data, give it a goddamned try\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[16000:16596], y[16000:16596], random_state=0)\n",
    "        regr = RandomForestRegressor(max_depth=8, random_state=0)\n",
    "        regr.fit(X_train, y_train)\n",
    "        \n",
    "        # Use the forest's predict method on the test data\n",
    "        predictions = regr.predict(X_test)\n",
    "        # Calculate the absolute errors\n",
    "        errors = abs(predictions - y_test)\n",
    "        # Calculate mean absolute percentage error (MAPE)\n",
    "        mape = 100 * (errors / y_test)\n",
    "        # Calculate and display accuracy\n",
    "        accuracy = 100 - np.mean(mape)\n",
    "        print('Accuracy:', round(accuracy, 2), '%.')\n",
    "        scores_rf.append(accuracy)\n",
    "\n",
    "        \n",
    "plt.title('mean squared error based on k thousands of rows of training iteratively\\nred for random forest, blue for grad boost')\n",
    "plt.plot(np.arange(1,18), scores_gb,color='blue')\n",
    "plt.plot(np.arange(1,18), scores_rf, color='red')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('mean errors')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
